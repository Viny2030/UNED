{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viny2030/UNED/blob/main/TP1_Analisis_de_sentimientos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "smid80_coronavirus_covid19_tweets_late_april_path = kagglehub.dataset_download('smid80/coronavirus-covid19-tweets-late-april')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "2Mr-shYcuJVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5497e82-9ed0-4181-f265-ac3f2c0b399a"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/smid80/coronavirus-covid19-tweets-late-april?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 946M/946M [00:20<00:00, 48.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "_IamYF9iuJV4"
      },
      "cell_type": "markdown",
      "source": [
        "# TP1 - Analisis de sentimientos\n",
        "Vamos a realizar un an√°lisis de sentimiento en base a los tweets obtenidos de la segunda quincena de abril de 2020 sobre el covid19."
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "uCU218BbuJV8"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "33cmTcyZuJV9"
      },
      "cell_type": "markdown",
      "source": [
        "## Uni√≥n y selecci√≥n de datos\n",
        "Lo primero que hacemos es unir todos los csv para poder procesarlos en un √∫nico archivo. Vamos a usar DataFrame de Pandas."
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "8uPCml-euJV-"
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "path = '/kaggle/input/coronavirus-covid19-tweets-late-april/'\n",
        "all_files = glob.glob(os.path.join(path, \"*.CSV\"))\n",
        "\n",
        "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
        "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0JUgXdzuJV-"
      },
      "cell_type": "markdown",
      "source": [
        "* Nos quedamos √∫nicamente con la columna del texto correspondiente al tweet, y del lenguaje:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lo6MT19guJV_"
      },
      "cell_type": "code",
      "source": [
        "df = concatenated_df[['text', 'lang']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rM6-M1AYuJV_"
      },
      "cell_type": "markdown",
      "source": [
        "* Luego nos quedamos solo con los tweets que est√°n en ingles (\"en\"):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-DXHBhDJuJWA"
      },
      "cell_type": "code",
      "source": [
        "filter = df.mask(lambda x: x['lang'] != 'en').dropna()\n",
        "filter.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGcivrPRuJWB"
      },
      "cell_type": "markdown",
      "source": [
        "* Eliminamos los tweet duplicados:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "uaFuhHTLuJWB"
      },
      "cell_type": "code",
      "source": [
        "texts = filter['text'].drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nXRDD-4JuJWC"
      },
      "cell_type": "markdown",
      "source": [
        "## Clasificaci√≥n de los datos\n",
        "En este punto ya tenemos en nuestro dataset 3.260.790 tweets.\n",
        "\n",
        "A continuaci√≥n vamos a extraer los que en sus textos contienen los iconos que representan los sentimientos para clasificar.\n",
        "\n",
        "* Comenzamos con los de \"Alegr√≠a\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "D9G6CPAzuJWC"
      },
      "cell_type": "code",
      "source": [
        "joy_icons = 'üòÅ|üòÇ|üòÉ|üòÑ|üòÖ|üòÜ|üòâ|üòä|üòç'\n",
        "\n",
        "joy = pd.Series(texts, dtype=\"string\", name=\"joy\").str.contains(joy_icons)\n",
        "joy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dGDx1iZuJWD"
      },
      "cell_type": "markdown",
      "source": [
        "* Continuamos con los de \"tristeza - miedo\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1FC3aTBquJWD"
      },
      "cell_type": "code",
      "source": [
        "sad_icons = 'üòì|üòñ|üò¢|üò≠|üò∞|üò±|üôç|üôé'\n",
        "\n",
        "sad = pd.Series(texts, dtype=\"string\", name=\"sad\").str.contains(sad_icons)\n",
        "sad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lZbnlCd5uJWE"
      },
      "cell_type": "markdown",
      "source": [
        "* Continuamos con los de \"enojo\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fqTbQnjTuJWE"
      },
      "cell_type": "code",
      "source": [
        "angry_icons = 'üò†|üò°|üò§|ü§¨|üëø|üíÄ|‚ò†'\n",
        "\n",
        "angry = pd.Series(texts, dtype=\"string\", name=\"angry\").str.contains(angry_icons)\n",
        "angry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHA3NatyuJWE"
      },
      "cell_type": "markdown",
      "source": [
        "* Y terminamos con los de \"apoyo - esperanza\":"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tC9Z3ZfAuJWF"
      },
      "cell_type": "code",
      "source": [
        "hope_icons = 'üò∑|üôã|üôå|üôè'\n",
        "\n",
        "hope = pd.Series(texts, dtype=\"string\", name=\"hope\").str.contains(hope_icons)\n",
        "hope"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BV2hKM7_uJWF"
      },
      "cell_type": "markdown",
      "source": [
        "* Unimos los resultados en un nuevo data frame:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "05ZhD09kuJWF"
      },
      "cell_type": "code",
      "source": [
        "df = pd.concat([texts, joy, sad, angry, hope], axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQwNs8MXuJWG"
      },
      "cell_type": "markdown",
      "source": [
        "* Ahora dividimos el dataset en 2, por un lado los que tienen alguna categor√≠a, y por otro los que no tienen ninguna:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CawthshfuJWG"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = df[(df['joy'] | df['sad'] | df['angry'] | df['hope'])]\n",
        "texts_not_classified = df[~(df['joy'] | df['sad'] | df['angry'] | df['hope'])]['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yiBpLx7nuJWG"
      },
      "cell_type": "markdown",
      "source": [
        "* Con los que estan clasificados, tenemos que limpiar los que tienen m√°s de 1 categor√≠a. Para eso vamos a sumar la cantidad de categorias en una nueva columna y quedarnos unicamente con los que tienen 1:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fY1Iji0EuJWG"
      },
      "cell_type": "code",
      "source": [
        "count = texts_classified.apply(lambda row: row['joy'] + row['sad'] + row['angry'] + row['hope'], axis=1)\n",
        "count = count.rename(\"count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TJoy4N7duJWG"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = pd.concat([texts_classified, count], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OP7rXn8duJWH"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = texts_classified[texts_classified['count'] == 1]\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "719o6TJOuJWH"
      },
      "cell_type": "markdown",
      "source": [
        "* Ahora en base a lo obtenido, vamos a etiquetar seg√∫n qu√© categor√≠a sean, los resultados ir√°n en la columna 'target':"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YIznQeY9uJWH"
      },
      "cell_type": "code",
      "source": [
        "def label_feeling (row):\n",
        "   if row['joy'] :\n",
        "      return 'JOY'\n",
        "   if row['sad'] :\n",
        "      return 'SAD'\n",
        "   if row['angry'] :\n",
        "      return 'ANGRY'\n",
        "   if row['hope'] :\n",
        "      return 'HOPE'\n",
        "   return 'NONE'\n",
        "\n",
        "texts_classified['target'] = texts_classified.apply(lambda row: label_feeling(row), axis=1)\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QHzO_OcuJWH"
      },
      "cell_type": "markdown",
      "source": [
        "* Y finalmente nos quedamos solo con las columnas 'text' y 'target', as√≠ nos queda nuestro set de datos clasificado:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "y4Agzr-LuJWH"
      },
      "cell_type": "code",
      "source": [
        "texts_classified = texts_classified[['text', 'target']]\n",
        "texts_classified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-_Do8WKuJWI"
      },
      "cell_type": "markdown",
      "source": [
        "## Limpieza de datos\n",
        "Antes de entrenar nuestro algoritmo, vamos a hacer una limpieza de datos a los textos de los tweets"
      ]
    },
    {
      "metadata": {
        "id": "cZVtcU9wuJWI"
      },
      "cell_type": "markdown",
      "source": [
        "* Primero vamos a aplicar una limpieza con regex para dejar solo las palabras sin puntuaciones ni n√∫meros:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Xrk5eiSouJWI"
      },
      "cell_type": "code",
      "source": [
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "\n",
        "    # remove numbers\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jS3dyFqAuJWJ"
      },
      "cell_type": "code",
      "source": [
        "clean_text(texts_classified, 'text')\n",
        "texts_classified.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "km-xcefKuJWJ"
      },
      "cell_type": "markdown",
      "source": [
        "* Luego vamos a eliminar todas las \"stop words\" para que queden las palabras m√°s importantes:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "PYH1Z3JauJWJ"
      },
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "\n",
        "texts_classified['text'] = texts_classified['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "texts_classified"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vp51vCd0uJWK"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento\n",
        "Vamos ahora a pasar a la etapa de entrenamiento, a continuaci√≥n detallamos los valores actuales"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_C-7P5lVuJWK"
      },
      "cell_type": "code",
      "source": [
        "texts_classified['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-zJ9cJKuJWS"
      },
      "cell_type": "markdown",
      "source": [
        "El dataset clasificado cuenta con 104.232 tweets. De los cuales tenemos:\n",
        "\n",
        "* HOPE con 48.484\n",
        "* JOY con 36.174\n",
        "* SAD con 10.660\n",
        "* ANGRY con 8.914"
      ]
    },
    {
      "metadata": {
        "id": "A3gl7qUTuJWT"
      },
      "cell_type": "markdown",
      "source": [
        "Como vemos, los datos est√°n desbalanceados, entonces procedemos a acortar el dataset para poder hacer un entrenamiento balanceado.\n",
        "\n",
        "Tomamos como n√∫mero m√†ximo el de **ANGRY** ya que es el m√°s chico:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JPmbsMBquJWT"
      },
      "cell_type": "code",
      "source": [
        "hope_target = texts_classified[texts_classified['target'] == 'HOPE'].sample(8914)\n",
        "joy_target = texts_classified[texts_classified['target'] == 'JOY'].sample(8914)\n",
        "sad_target = texts_classified[texts_classified['target'] == 'SAD'].sample(8914)\n",
        "angry_target = texts_classified[texts_classified['target'] == 'ANGRY'].sample(8914)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CYuiQ3AZuJWT"
      },
      "cell_type": "code",
      "source": [
        "balanced_data = pd.concat([hope_target, joy_target, sad_target, angry_target], ignore_index=True)\n",
        "balanced_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HF27pwYjuJWU"
      },
      "cell_type": "markdown",
      "source": [
        "* Con este dataset reducido, dividimos en datos de entrenamiento (66%) y datos de prueba (33%), sin perder el balance (para eso se usa stratify):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gS8APxgvuJWU"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(balanced_data['text'], balanced_data['target'], test_size=0.33, random_state=0, stratify=balanced_data['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RcLHvh7guJWU"
      },
      "cell_type": "markdown",
      "source": [
        "* Preparamos el pipeline con el algoritmo de Naive Bayes:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aOVt8HBGuJWV"
      },
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8xST-1E5uJWW"
      },
      "cell_type": "markdown",
      "source": [
        "* Y lo entrenamos con el dataset de entrenamiento:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "NrGYilxkuJWX"
      },
      "cell_type": "code",
      "source": [
        "text_clf = text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mOyOt1YSuJWX"
      },
      "cell_type": "markdown",
      "source": [
        "* Finalmente, predecimos en base a los datos de prueba y evaluamos los resultados:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Aw41ex9zuJWY"
      },
      "cell_type": "code",
      "source": [
        "predicted = text_clf.predict(X_test)\n",
        "print(classification_report(y_test, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kr76vkpouJWY"
      },
      "cell_type": "markdown",
      "source": [
        "* Y as√≠ obtenemos los siguientes resultados:"
      ]
    },
    {
      "metadata": {
        "id": "hUz35xgquJWZ"
      },
      "cell_type": "markdown",
      "source": [
        "                precision    recall  f1-score   support\n",
        "\n",
        "       ANGRY       0.33      0.82      0.47      2942\n",
        "        HOPE       0.81      0.61      0.70     16000\n",
        "         JOY       0.71      0.50      0.58     11937\n",
        "         SAD       0.35      0.66      0.45      3518\n",
        "\n",
        "    accuracy                           0.59     34397\n",
        "    macro avg      0.55      0.65      0.55     34397\n",
        "    weighted avg   0.69      0.59      0.61     34397"
      ]
    },
    {
      "metadata": {
        "id": "rPcCT6EhuJWa"
      },
      "cell_type": "markdown",
      "source": [
        "Como conclusi√≥n de este entrenamiento, podemos decir que no son los resultados que esperabamos, en el medio se intentaron diversas tecnicas de limpieza de datos que no aportaron mucha mejora. El problema base recae en la cantidad de datos usados para el entrenamiento, ya que el tener que rebalancear, me vi obligado a desprenderme de ciertos datos ya clasificados. La tendencia de clases al rebalancear termina siendo distinta a la obtenida a partir de los emojis, esto tambi√©n puede influir cuando pasemos a la predicci√≥n de los tweets sin clasificar."
      ]
    },
    {
      "metadata": {
        "id": "qwe_Udt2uJWb"
      },
      "cell_type": "markdown",
      "source": [
        "La ditribuci√≥n nos quedo:\n",
        "* HOPE con 11996\n",
        "* JOY con 8334\n",
        "* ANGRY con 7335\n",
        "* SAD con 6732"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hLmxlapEuJWb"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(predicted).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdHp_ALluJWd"
      },
      "cell_type": "markdown",
      "source": [
        "## Predicci√≥n\n",
        "Pasamos a predecir los tweets que nos quedaron sin clasificar. En total son 3.150.070.\n",
        "\n",
        "* Primero que nada, limpiamos los datos de igual manera que los de entrenamiento:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kOiogTeEuJWd"
      },
      "cell_type": "code",
      "source": [
        "clean_data = texts_not_classified.str.lower()\n",
        "clean_data = clean_data.apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "clean_data = clean_data.apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ekIVWZ9yuJWe"
      },
      "cell_type": "code",
      "source": [
        "clean_data = clean_data.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "clean_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CLWkBIsyuJWe"
      },
      "cell_type": "markdown",
      "source": [
        "* Una vez preparados, pasamos a la predicci√≥n, utilizando el algoritmo ya entrena previamente:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8EpnxVgpuJWe"
      },
      "cell_type": "code",
      "source": [
        "results = text_clf.predict(clean_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZlMVSbtuJWf"
      },
      "cell_type": "markdown",
      "source": [
        "* Y vemos los resultados;"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4rgh9JsvuJWf"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(results).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m32WbUDHuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "Los resultados finales son:\n",
        "\n",
        "* ANGRY con 1.169.389 (37,12%)\n",
        "* HOPE con 1.042.784 (33,1%)\n",
        "* SAD con 528.640 (16,78%)\n",
        "* JOY con 409.257 (12,99%)\n",
        "\n",
        "Los tweets positivos representan el 46.1%, mientras que los negativos el 53,9%"
      ]
    },
    {
      "metadata": {
        "id": "S_3S315SuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "A primera vista, si comparamos con el balance de los tweets clasificados, no hay correlaci√≥n. En estos, hay mayor tendencia hacia a los negativos, en cambio en los de entrenamiento era hacia los positivos. Ahora vamos a comparar contra otro algoritmo."
      ]
    },
    {
      "metadata": {
        "id": "FTzdugIcuJWg"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 2\n",
        "Para esta parte utilizamos el algoritmo sacado de la p√°gina https://nlpforhackers.io/sentiment-analysis-intro/ que utiliza la libreria de SentiWordNet de Natural Language ToolKits.\n",
        "\n",
        "Previo a eso hace un preprocesamiento de los datos, con lematizaci√≥n de por medio."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Pi4mp2IJuJWg"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    \"\"\"\n",
        "    Convert between the PennTreebank tags to simple Wordnet tags\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"<br />\", \" \")\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def swn_polarity(text):\n",
        "    \"\"\"\n",
        "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
        "    \"\"\"\n",
        "\n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        "\n",
        "    text = clean_text(text)\n",
        "\n",
        "\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    for raw_sentence in raw_sentences:\n",
        "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
        "\n",
        "        for word, tag in tagged_sentence:\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "\n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "            if not lemma:\n",
        "                continue\n",
        "\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        "\n",
        "            # Take the first sense, the most common\n",
        "            synset = synsets[0]\n",
        "            swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "            tokens_count += 1\n",
        "\n",
        "    # judgment call ? Default to positive or negative\n",
        "    if not tokens_count:\n",
        "        return 0\n",
        "\n",
        "    # sum greater than 0 => positive sentiment\n",
        "    if sentiment >= 0:\n",
        "        return 1\n",
        "\n",
        "    # negative sentiment\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "87Z9oYdBuJWi"
      },
      "cell_type": "markdown",
      "source": [
        "* Corremos el algoritmo (no recomiendo correr de nuevo, ya que no est√° optimizado y tom√≥ mucho tiempo llegar a resultados):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WSpLzL7KuJWi"
      },
      "cell_type": "code",
      "source": [
        "pred_y = [swn_polarity(text) for text in clean_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fRFlEE3muJWi"
      },
      "cell_type": "code",
      "source": [
        "pd.Series(pred_y).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECgIipsCuJWj"
      },
      "cell_type": "markdown",
      "source": [
        "Los resultados obtenidos fueron:\n",
        "\n",
        "* Positivos: 2.205.853 (70,03%)\n",
        "* Negativos: 944.217 (29.97%)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1dAIGMQUuJWj"
      },
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, los datos obtenidos con el SentiWordNet, tienen mucha mas correlaci√≥n con los obtenidos y clasificados a partir de los emojis. Esto quiere decir, que estabamos en lo correcto cuando deciamos que nuestro algoritmo de naive bayes no nos hab√≠a dado muy buenos resultados, a pesar de tener una precisi√≥n cercana al 70% en la prueba."
      ]
    },
    {
      "metadata": {
        "id": "T3qmE-nyuJWk"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusi√≥n\n",
        "A pesar de no poder lograr mejorar el algoritmo de naive bayes, siento que se hizo un buen trabajo con el manejo y limpieza de datos. Quizas se tendr√≠a que haber profundizado m√°s en la identificaci√≥n de elementos dentro de los tweets. M√°s all√° de eso, los resultados a primera vista no parec√≠an ser tan malos, las comparaciones finales demostraron lo contrario.\n",
        "\n",
        "Podemos concluir que una de las causas raices de la mala performance fue la baja cantidad de datos negativos m√°s que nada. Eso causo un desbalanceo en los datos de entrenamiento. A pesa de que agregamos emojis, no pudimos subir los porcentajes de clases dentro de los clasificados. Esto nos llevo a reducir datos de entrenamiento para poder entrenar el algoritmo de manera balanceada.\n",
        "\n",
        "Como mejora a futuro, probar√≠a correr de vuelta el algoritmo pero sin rebalancear las clases, teniendo en cuenta que es muy probable que la tendencia que sigan los tweets sin clasificar se correlacione con los clasificados. Esto qued√≥ demostrado en parte al correr el algoritmo de SentiWordNet."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}